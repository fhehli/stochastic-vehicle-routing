{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup and Dependency Installation\n",
    "Don't run this if you don't have problems with dataset creation (sectino below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!source .venv/bin/activate\n",
    "%pip install -r requirements.txt --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "First create the dataset using the command below. If the command fails, try running it directly in the terminal instead.\n",
    "After the dataset is created, adapt the config file in `configs`. The default is `configs/test.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "!python3 src/dataset_creator.py --city --n_samples 5 --n_tasks 4 --out_file data/test.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Interactive Trainer\n",
    "This is an adaption of `src/trainer.py`. It can be used to log the city instances and the VSP solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from src.VSPSolver import solve_vsp\n",
    "from src.utils import get_model, get_criterion, get_optimizer, get_dataloaders\n",
    "from src.city import City\n",
    "from src.visualization_utils import visualize_paths, visualize_tasks\n",
    "\n",
    "\n",
    "# add simple tensorboard to the trainer to visualize results \n",
    "class InteractiveTrainer:\n",
    "    def __init__(self, config, log=False, **kwargs):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = get_model(config, device=self.device)\n",
    "        self.train_loader, self.val_loader, self.test_loader = get_dataloaders(config)\n",
    "\n",
    "        optimizer_class, optimizer_kwargs = get_optimizer(config)\n",
    "        self.optimizer = optimizer_class(self.model.parameters(), **optimizer_kwargs)\n",
    "\n",
    "        criterion_class, criterion_kwargs = get_criterion(config)\n",
    "        self.criterion = lambda func: criterion_class(func, **criterion_kwargs)\n",
    "\n",
    "        self.n_epochs = config[\"train\"][\"n_epochs\"]\n",
    "        self.eval_every = config[\"train\"][\"eval_every_n_epochs\"]\n",
    "        self.save_every = config[\"train\"][\"save_every_n_epochs\"]\n",
    "        self.save_dir = Path(config[\"train\"][\"save_dir\"])\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.with_city = config[\"data\"][\"city\"]\n",
    "        self.log = log\n",
    "        self.writer = SummaryWriter(kwargs[\"logdir\"]) if self.log else None\n",
    "\n",
    "    # compare all paths to chosen path and SVSP path\n",
    "    def update_log(self, instance, iter: int, loss: float, solution: list[int], labels: list[int], **kwargs):\n",
    "        if isinstance(instance, City):\n",
    "            # figure = visualize_paths(instance, solution)\n",
    "            solution_figure = visualize_tasks(instance, vsp_path=solution)\n",
    "            gt_figure = visualize_tasks(instance, vsp_path=labels)\n",
    "            self.writer.add_figure(f\"City/iter {iter}: solution\", solution_figure)\n",
    "            self.writer.add_figure(f\"City/iter {iter}: ground truth\", gt_figure)\n",
    "        self.writer.add_scalar(f\"Loss\", loss, iter)\n",
    "\n",
    "    def compute_metrics(self, i):\n",
    "        if i % self.eval_every != 0:\n",
    "            return\n",
    "        losses = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, instance in self.val_loader:\n",
    "                graph = instance.graph if self.with_city else instance\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                theta = self.model(inputs)\n",
    "                func = partial(solve_vsp, graph=graph)\n",
    "                criterion = self.criterion(func)\n",
    "                loss = criterion(theta, labels).mean()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        print(f\"Validation loss: {np.mean(losses):.3f}\")\n",
    "\n",
    "    def save_model(self, i):\n",
    "        if i % self.save_every == 0 and i > 0:\n",
    "            torch.save(self.model.state_dict(), self.save_dir / f\"epoch{i}.pt\")\n",
    "\n",
    "    def train_epoch(self, i):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        for iter, (inputs, labels, instance) in tqdm(enumerate(self.train_loader), desc=f\"Epoch {i}\"):\n",
    "            graph = instance.graph if self.with_city else instance\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            theta = self.model(inputs)\n",
    "\n",
    "            func = partial(solve_vsp, graph=graph)\n",
    "            criterion = self.criterion(func)\n",
    "            loss = criterion(theta, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # workaround to get single solution from the solver\n",
    "            solution = func(theta.detach().clone().unsqueeze(0))[0]\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # visual logging here\n",
    "            if self.log:\n",
    "                self.global_steps += 1\n",
    "                self.update_log(instance, self.global_steps, loss, solution, labels)\n",
    "\n",
    "        print(f\"Train loss: {np.mean(losses):.3f}\")\n",
    "\n",
    "    def train(self):\n",
    "        self.global_steps = 0\n",
    "        for i in range(self.n_epochs):\n",
    "            self.train_epoch(i)\n",
    "            self.compute_metrics(i)\n",
    "            self.save_model(i)\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, instance in tqdm(self.test_loader):\n",
    "                graph = instance.graph if self.with_city else instance\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                theta = self.model(inputs)\n",
    "                func = partial(solve_vsp, graph=graph)\n",
    "                criterion = self.criterion(func)\n",
    "                loss = criterion(theta, labels)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        print(f\"Test loss: {np.mean(losses):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the dataset\n",
    "Run the following to start training our model. The figures will be stored in `runs` by default. Use `tensorboard --logdir=runs` to view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"configs/test.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    trainer = InteractiveTrainer(config, log=False, logdir=\"runs\")\n",
    "    trainer.train()\n",
    "    trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset \n",
    "Check for city instances where the task is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.city import SimpleDirectedGraph, Vertex, Edge\n",
    "\n",
    "def get_outgoing_edges(vertex: Vertex, graph: SimpleDirectedGraph, sol: list[int]) -> list[Edge]:\n",
    "    result = []\n",
    "\n",
    "    for i, e in enumerate(graph.get_edges()):\n",
    "        if e.from_vertex == vertex and sol[i] == 1:\n",
    "            result.append(e)\n",
    "\n",
    "    return result\n",
    "\n",
    "def check_easy_problems(data_path: str, with_city: bool):\n",
    "    with open(data_path, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "        X = data[\"X\"]\n",
    "        Y = data[\"Y\"]\n",
    "        graphs: SimpleDirectedGraph = list(map(lambda city: city.graph, data[\"cities\"])) if with_city else data[\"graphs\"]\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i, (g, s) in enumerate(zip(graphs, Y)):\n",
    "        if len(get_outgoing_edges(g.get_source(), g, s)) == len(g.get_vertices())-2 or \\\n",
    "           len(get_outgoing_edges(g.get_source(), g, s)) == 1:\n",
    "            counter += 1\n",
    "\n",
    "    print(f\"percentage easy problems: {counter/len(graphs)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage easy problems: 70.0\n"
     ]
    }
   ],
   "source": [
    "check_easy_problems(\"data/test.pkl\", with_city=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
